{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc55489",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning and AI in Python\n",
    "\n",
    "This notebook covers advanced machine learning and AI concepts using Python. You'll learn about ensemble methods, model selection, deep learning basics, natural language processing (NLP), and model deployment.\n",
    "\n",
    "## Topics Covered:\n",
    "1. Ensemble Methods (Bagging, Boosting, Random Forest, Gradient Boosting)\n",
    "2. Model Selection and Hyperparameter Tuning\n",
    "3. Introduction to Deep Learning (Neural Networks)\n",
    "4. Natural Language Processing (NLP) Basics\n",
    "5. Model Deployment and Serving\n",
    "6. Real-Life Use Cases and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75722954",
   "metadata": {},
   "source": [
    "## 1. Ensemble Methods\n",
    "\n",
    "Ensemble methods combine multiple models to improve performance and robustness. Popular techniques include bagging (e.g., Random Forest) and boosting (e.g., Gradient Boosting, AdaBoost).\n",
    "\n",
    "**Real-life use case:** Credit card fraud detection systems use ensemble models to reduce false positives and improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c38c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine(as_frame=True)\n",
    "X, y = wine.data, wine.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "print('Random Forest Accuracy:', accuracy_score(y_test, rf_pred))\n",
    "\n",
    "# Gradient Boosting\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "gb_pred = gb.predict(X_test)\n",
    "print('Gradient Boosting Accuracy:', accuracy_score(y_test, gb_pred))\n",
    "\n",
    "# AdaBoost\n",
    "ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "ada.fit(X_train, y_train)\n",
    "ada_pred = ada.predict(X_test)\n",
    "print('AdaBoost Accuracy:', accuracy_score(y_test, ada_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c70c6a4",
   "metadata": {},
   "source": [
    "## 2. Model Selection and Hyperparameter Tuning\n",
    "\n",
    "Selecting the best model and tuning its hyperparameters is crucial for optimal performance. Techniques include cross-validation and grid/randomized search.\n",
    "\n",
    "**Real-life use case:** Tuning a model for predicting loan defaults to maximize recall and minimize false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f47b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Grid search for Random Forest hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "print('Best parameters:', grid.best_params_)\n",
    "print('Best cross-validated score:', grid.best_score_)\n",
    "\n",
    "# Cross-validation example\n",
    "cv_scores = cross_val_score(RandomForestClassifier(**grid.best_params_, random_state=42), X, y, cv=5)\n",
    "print('Cross-validation scores:', cv_scores)\n",
    "print('Mean CV score:', cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221339e4",
   "metadata": {},
   "source": [
    "## 3. Introduction to Deep Learning (Neural Networks)\n",
    "\n",
    "Deep learning uses neural networks with multiple layers to learn complex patterns. Keras (with TensorFlow backend) is a popular library for building neural networks in Python.\n",
    "\n",
    "**Real-life use case:** Image recognition in self-driving cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb20de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Prepare data for neural network\n",
    "lb = LabelBinarizer()\n",
    "y_train_bin = lb.fit_transform(y_train)\n",
    "y_test_bin = lb.transform(y_test)\n",
    "\n",
    "# Build a simple neural network\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(len(lb.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train_bin, epochs=20, batch_size=8, verbose=0)\n",
    "loss, acc = model.evaluate(X_test, y_test_bin, verbose=0)\n",
    "print('Neural Network Test Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e7f6c",
   "metadata": {},
   "source": [
    "## 4. Natural Language Processing (NLP) Basics\n",
    "\n",
    "NLP enables computers to understand and process human language. Common tasks include text classification, sentiment analysis, and topic modeling.\n",
    "\n",
    "**Real-life use case:** Sentiment analysis of customer reviews for brand monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3079920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Example text data\n",
    "texts = ['I love this product', 'Worst experience ever', 'Amazing quality', 'Not good', 'Will buy again', 'Terrible support']\n",
    "labels = [1, 0, 1, 0, 1, 0]  # 1=positive, 0=negative\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_text = vectorizer.fit_transform(texts)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_text, labels)\n",
    "pred = clf.predict(X_text)\n",
    "print('Text Classification Accuracy:', accuracy_score(labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62de7db",
   "metadata": {},
   "source": [
    "## 5. Model Deployment and Serving\n",
    "\n",
    "Deploying models allows you to serve predictions in real-time or batch mode. Common tools include Flask, FastAPI, and cloud services.\n",
    "\n",
    "**Real-life use case:** Deploying a fraud detection model as a REST API for a banking application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00c35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple Flask API for model prediction (conceptual, not executable here)\n",
    "from flask import Flask, request, jsonify\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Assume clf is a trained model\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json(force=True)\n",
    "    features = np.array(data['features']).reshape(1, -1)\n",
    "    prediction = clf.predict(features)[0]\n",
    "    return jsonify({'prediction': int(prediction)})\n",
    "\n",
    "# To run: app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9270e506",
   "metadata": {},
   "source": [
    "## 6. Real-Life Use Cases and Best Practices\n",
    "\n",
    "- **Healthcare:** Deep learning for medical image analysis\n",
    "- **Finance:** NLP for automated document processing\n",
    "- **Retail:** Recommendation systems using collaborative filtering and deep learning\n",
    "- **Manufacturing:** Predictive maintenance with time series and anomaly detection\n",
    "- **Best Practices:**\n",
    "    - Always validate models with cross-validation\n",
    "    - Monitor deployed models for data drift\n",
    "    - Use explainable AI tools (e.g., SHAP, LIME) for model transparency\n",
    "    - Document and version your models for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b8f788",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "1. Try ensemble methods on a different dataset and compare results.\n",
    "2. Build and tune a neural network for a regression problem.\n",
    "3. Perform sentiment analysis on a set of real product reviews.\n",
    "4. Deploy a simple model using FastAPI or Flask.\n",
    "5. Explore explainable AI tools to interpret your model's predictions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
