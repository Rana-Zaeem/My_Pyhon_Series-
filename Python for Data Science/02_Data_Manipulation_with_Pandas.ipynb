{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1ddfccb",
   "metadata": {},
   "source": [
    "# Data Manipulation with Pandas\n",
    "\n",
    "This notebook covers essential data manipulation techniques using the pandas library in Python. Pandas is one of the most powerful and flexible tools for data analysis and manipulation in Python.\n",
    "\n",
    "## Topics Covered:\n",
    "1. Introduction to pandas and basic data structures\n",
    "2. Reading and writing data\n",
    "3. Data inspection and cleaning\n",
    "4. Filtering, selecting, and indexing data\n",
    "5. Data transformation and aggregation\n",
    "6. Handling missing data\n",
    "7. Merging, joining, and concatenating dataframes\n",
    "8. Working with time series data\n",
    "9. Practical exercises\n",
    "\n",
    "Each section includes real-life use cases to demonstrate practical applications of these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b88147f",
   "metadata": {},
   "source": [
    "## 1. Introduction to Pandas and Basic Data Structures\n",
    "\n",
    "Pandas provides two primary data structures:\n",
    "- **Series**: One-dimensional array-like object containing a sequence of values with associated labels (index)\n",
    "- **DataFrame**: Two-dimensional tabular data structure with labeled axes (rows and columns)\n",
    "\n",
    "### Real-Life Use Case: Customer Analytics\n",
    "\n",
    "Imagine you work for an e-commerce company and need to analyze customer data. You'd use a DataFrame to store information about each customer (rows) with various attributes like purchase history, demographics, and engagement metrics (columns). Series objects might represent individual metrics like customer lifetime value or days since last purchase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b29e790e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import pandas library\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Display settings for better readability\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Import pandas library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display settings for better readability\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b0218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Series\n",
    "s = pd.Series([10, 20, 30, 40], index=['a', 'b', 'c', 'd'])\n",
    "print(\"Pandas Series:\")\n",
    "print(s)\n",
    "print(\"\\nIndex:\", s.index)\n",
    "print(\"Values:\", s.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2adc274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame\n",
    "data = {\n",
    "    'Name': ['John', 'Anna', 'Peter', 'Linda'],\n",
    "    'Age': [28, 34, 29, 42],\n",
    "    'City': ['New York', 'Paris', 'Berlin', 'London'],\n",
    "    'Salary': [65000, 70000, 62000, 85000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"DataFrame:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f748d",
   "metadata": {},
   "source": [
    "## 2. Reading and Writing Data\n",
    "\n",
    "Pandas can read data from various file formats including CSV, Excel, SQL databases, JSON, and more.\n",
    "\n",
    "### Real-Life Use Case: Financial Data Analysis\n",
    "\n",
    "In financial analysis, you often need to import data from multiple sources. For example, you might download historical stock price CSVs from Yahoo Finance, import transaction records from Excel spreadsheets, and pull economic indicators from an API in JSON format. Pandas provides a consistent interface for importing and exporting all these formats, allowing you to focus on the analysis rather than dealing with the complexities of different file types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab7b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sample DataFrame to save\n",
    "sample_df = pd.DataFrame({\n",
    "    'A': np.random.rand(5),\n",
    "    'B': np.random.randint(0, 10, 5),\n",
    "    'C': ['foo', 'bar', 'baz', 'qux', 'quux'],\n",
    "    'D': pd.date_range('2023-01-01', periods=5)\n",
    "})\n",
    "\n",
    "# Saving to CSV\n",
    "sample_df.to_csv('sample_data.csv', index=False)\n",
    "print(\"Data saved to CSV file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ea6a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from CSV\n",
    "df_from_csv = pd.read_csv('sample_data.csv')\n",
    "print(\"Data loaded from CSV:\")\n",
    "df_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5988469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a more realistic sample dataset\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2023-01-01', periods=100)\n",
    "\n",
    "sales_data = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Store': np.random.choice(['A', 'B', 'C', 'D'], 100),\n",
    "    'Product': np.random.choice(['Widget', 'Gadget', 'Tool', 'Device'], 100),\n",
    "    'Units_Sold': np.random.randint(1, 50, 100),\n",
    "    'Revenue': np.random.randint(100, 5000, 100),\n",
    "    'Customer_Satisfaction': np.random.randint(1, 6, 100)\n",
    "})\n",
    "\n",
    "# Save this dataset for future use\n",
    "sales_data.to_csv('sales_data.csv', index=False)\n",
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385dc8a3",
   "metadata": {},
   "source": [
    "## 3. Data Inspection and Cleaning\n",
    "\n",
    "Before analyzing data, it's important to inspect and clean it by checking its structure, identifying missing values, and handling duplicates.\n",
    "\n",
    "### Real-Life Use Case: Healthcare Data Management\n",
    "\n",
    "In healthcare organizations, patient data is often collected from various departments and systems, leading to inconsistencies, missing values, and duplicates. Clean data is essential for accurate diagnoses, treatment planning, and billing. Data scientists working with electronic health records (EHR) must carefully inspect and clean the data to ensure patient information is accurate and complete before using it for operational analytics or predictive modeling. For example, identifying and addressing missing lab results or duplicate patient records can significantly impact healthcare outcomes and financial reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc52a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sales data\n",
    "df = pd.read_csv('sales_data.csv')\n",
    "\n",
    "# Basic information about the DataFrame\n",
    "print(\"Shape (rows, columns):\", df.shape)\n",
    "print(\"\\nColumn names:\", df.columns.tolist())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e59c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Let's introduce some missing values for demonstration\n",
    "df_with_missing = df.copy()\n",
    "df_with_missing.loc[np.random.choice(df.index, 10), 'Revenue'] = np.nan\n",
    "df_with_missing.loc[np.random.choice(df.index, 5), 'Customer_Satisfaction'] = np.nan\n",
    "\n",
    "print(\"\\nMissing values in the modified dataset:\")\n",
    "print(df_with_missing.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a0c397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "# Method 1: Fill missing values with mean/median/mode\n",
    "df_filled = df_with_missing.copy()\n",
    "df_filled['Revenue'] = df_filled['Revenue'].fillna(df_filled['Revenue'].mean())\n",
    "df_filled['Customer_Satisfaction'] = df_filled['Customer_Satisfaction'].fillna(\n",
    "    df_filled['Customer_Satisfaction'].median())\n",
    "\n",
    "# Method 2: Drop rows with missing values\n",
    "df_dropped = df_with_missing.dropna()\n",
    "\n",
    "print(f\"Original shape: {df_with_missing.shape}\")\n",
    "print(f\"After filling: {df_filled.shape}\")\n",
    "print(f\"After dropping: {df_dropped.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b8b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting and handling duplicates\n",
    "# Let's introduce some duplicates\n",
    "df_with_duplicates = pd.concat([df, df.iloc[:5]], ignore_index=True)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_count = df_with_duplicates.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Remove duplicates\n",
    "df_unique = df_with_duplicates.drop_duplicates()\n",
    "print(f\"Shape after removing duplicates: {df_unique.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce9ff6",
   "metadata": {},
   "source": [
    "## 4. Filtering, Selecting, and Indexing Data\n",
    "\n",
    "Pandas provides powerful ways to select, filter, and access data within DataFrames.\n",
    "\n",
    "### Real-Life Use Case: Marketing Campaign Analysis\n",
    "\n",
    "A marketing team needs to segment customers and target specific campaigns. Using pandas filtering capabilities, they can quickly isolate high-value customers (e.g., `df[df['lifetime_value'] > 10000]`), identify inactive users (e.g., `df[df['days_since_last_purchase'] > 90]`), or target specific demographics (e.g., `df[(df['age'] > 25) & (df['age'] < 40) & (df['location'] == 'New York')]`). These filtered segments form the basis for tailored marketing strategies and personalized communications, significantly improving campaign performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a36ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic column selection\n",
    "print(\"Selecting a single column as Series:\")\n",
    "print(df['Revenue'].head())\n",
    "\n",
    "print(\"\\nSelecting multiple columns as DataFrame:\")\n",
    "print(df[['Store', 'Product', 'Revenue']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb59392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row selection using iloc (position-based) and loc (label-based)\n",
    "print(\"First 5 rows using iloc:\")\n",
    "print(df.iloc[:5])\n",
    "\n",
    "print(\"\\nRows 10-15:\")\n",
    "print(df.iloc[10:16])\n",
    "\n",
    "print(\"\\nSpecific rows and columns using iloc:\")\n",
    "print(df.iloc[0:3, [1, 3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada4d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering data based on conditions\n",
    "# Filter stores with high revenue\n",
    "high_revenue = df[df['Revenue'] > 4000]\n",
    "print(\"High revenue transactions:\")\n",
    "print(high_revenue)\n",
    "\n",
    "# Filter specific store and product combinations\n",
    "store_a_widgets = df[(df['Store'] == 'A') & (df['Product'] == 'Widget')]\n",
    "print(\"\\nStore A Widget sales:\")\n",
    "print(store_a_widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8174c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using query method for more readable filtering\n",
    "high_satisfaction = df.query('Customer_Satisfaction >= 4 & Units_Sold > 30')\n",
    "print(\"High satisfaction and high volume sales:\")\n",
    "print(high_satisfaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1cad60",
   "metadata": {},
   "source": [
    "## 5. Data Transformation and Aggregation\n",
    "\n",
    "Pandas provides various methods for transforming, grouping, and aggregating data.\n",
    "\n",
    "### Real-Life Use Case: Retail Sales Analysis\n",
    "\n",
    "A national retail chain needs to understand sales performance across different stores, regions, and product categories. Using pandas aggregation functions, analysts can quickly summarize total sales by region (`df.groupby('region')['sales'].sum()`), calculate average daily transactions per store (`df.groupby(['store', 'date'])['transactions'].mean()`), or identify the top-selling products in each category (`df.groupby(['category', 'product'])['units_sold'].sum().groupby(level=0).nlargest(3)`). These insights help management make informed decisions about inventory management, store staffing, and marketing resource allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8169f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new columns\n",
    "df['Revenue_per_Unit'] = df['Revenue'] / df['Units_Sold']\n",
    "df['Is_High_Value'] = df['Revenue'] > df['Revenue'].median()\n",
    "\n",
    "# Apply a custom function to a column\n",
    "def categorize_satisfaction(score):\n",
    "    if score >= 4:\n",
    "        return 'High'\n",
    "    elif score >= 2:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "df['Satisfaction_Category'] = df['Customer_Satisfaction'].apply(categorize_satisfaction)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242bb232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by operations\n",
    "store_summary = df.groupby('Store').agg({\n",
    "    'Revenue': ['sum', 'mean'], \n",
    "    'Units_Sold': 'sum',\n",
    "    'Customer_Satisfaction': 'mean'\n",
    "})\n",
    "\n",
    "print(\"Store summary:\")\n",
    "store_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10132612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-level groupby\n",
    "product_store_summary = df.groupby(['Store', 'Product']).agg({\n",
    "    'Revenue': 'sum',\n",
    "    'Units_Sold': 'sum',\n",
    "    'Customer_Satisfaction': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "print(\"Product and store summary:\")\n",
    "product_store_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24448c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot tables\n",
    "pivot_table = df.pivot_table(\n",
    "    values=['Revenue', 'Units_Sold'],\n",
    "    index='Store',\n",
    "    columns='Product',\n",
    "    aggfunc={'Revenue': 'sum', 'Units_Sold': 'sum'}\n",
    ")\n",
    "\n",
    "print(\"Pivot table of revenue and units sold by store and product:\")\n",
    "pivot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f32b362",
   "metadata": {},
   "source": [
    "## 6. Handling Missing Data\n",
    "\n",
    "Let's explore more advanced techniques for handling missing data.\n",
    "\n",
    "### Real-Life Use Case: Environmental Sensor Data\n",
    "\n",
    "Environmental scientists deploy networks of sensors to monitor air quality, temperature, humidity, and other metrics. These sensors occasionally fail or experience connectivity issues, resulting in gaps in the data. When analyzing trends or building predictive models, these gaps must be addressed. Using pandas, scientists can apply domain-appropriate techniques such as linear interpolation for temperature readings, forward-filling for slowly changing metrics like humidity, or more sophisticated techniques like time-series-based imputation for complex relationships. Proper handling of missing sensor data ensures more accurate environmental modeling and reliable alerting systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb853c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with various missing patterns\n",
    "df_missing = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [np.nan, 2, 3, 4, 5],\n",
    "    'C': [1, 2, 3, np.nan, np.nan],\n",
    "    'D': [1, 2, 3, 4, 5]\n",
    "})\n",
    "\n",
    "print(\"Original dataset with missing values:\")\n",
    "print(df_missing)\n",
    "print(\"\\nMissing value count by column:\")\n",
    "print(df_missing.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78583442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different imputation strategies\n",
    "\n",
    "# 1. Fill with a constant value\n",
    "df_fill_const = df_missing.fillna(0)\n",
    "print(\"Filled with constant value:\")\n",
    "print(df_fill_const)\n",
    "\n",
    "# 2. Fill with different values for each column\n",
    "df_fill_dict = df_missing.fillna({'A': 0, 'B': 10, 'C': -1})\n",
    "print(\"\\nFilled with different values per column:\")\n",
    "print(df_fill_dict)\n",
    "\n",
    "# 3. Forward fill (propagate last valid observation forward)\n",
    "df_ffill = df_missing.fillna(method='ffill')\n",
    "print(\"\\nForward fill:\")\n",
    "print(df_ffill)\n",
    "\n",
    "# 4. Backward fill (use next valid observation to fill gap)\n",
    "df_bfill = df_missing.fillna(method='bfill')\n",
    "print(\"\\nBackward fill:\")\n",
    "print(df_bfill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4888a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation methods\n",
    "df_interp = df_missing.interpolate(method='linear')\n",
    "print(\"Linear interpolation:\")\n",
    "print(df_interp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a533d44",
   "metadata": {},
   "source": [
    "## 7. Merging, Joining, and Concatenating DataFrames\n",
    "\n",
    "Pandas provides various ways to combine multiple DataFrames together.\n",
    "\n",
    "### Real-Life Use Case: Supply Chain Management\n",
    "\n",
    "In supply chain management, data is often siloed across different systems. A manufacturing company might have separate datasets for inventory levels, production schedules, supplier information, transportation logistics, and customer orders. To optimize operations, these datasets need to be combined. Using pandas, analysts can join supplier data with inventory levels, merge production schedules with materials availability, and link shipping data with customer orders. These combined datasets enable end-to-end visibility, allowing managers to identify bottlenecks, optimize inventory levels, reduce lead times, and improve overall supply chain efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960b0d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrames\n",
    "df1 = pd.DataFrame({\n",
    "    'ID': ['A1', 'A2', 'A3', 'A4'],\n",
    "    'Name': ['John', 'Emily', 'Martha', 'Samuel'],\n",
    "    'Department': ['HR', 'Marketing', 'Finance', 'IT']\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'ID': ['A2', 'A3', 'A4', 'A5'],\n",
    "    'Salary': [60000, 80000, 70000, 90000],\n",
    "    'Years_Employed': [3, 7, 4, 2]\n",
    "})\n",
    "\n",
    "df3 = pd.DataFrame({\n",
    "    'Department': ['HR', 'Marketing', 'Finance', 'IT', 'Operations'],\n",
    "    'Budget': [100000, 200000, 300000, 250000, 150000]\n",
    "})\n",
    "\n",
    "print(\"DataFrame 1:\")\n",
    "print(df1)\n",
    "print(\"\\nDataFrame 2:\")\n",
    "print(df2)\n",
    "print(\"\\nDataFrame 3:\")\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac2f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different join types\n",
    "\n",
    "# Inner join\n",
    "inner_join = pd.merge(df1, df2, on='ID', how='inner')\n",
    "print(\"Inner join:\")\n",
    "print(inner_join)\n",
    "\n",
    "# Left join\n",
    "left_join = pd.merge(df1, df2, on='ID', how='left')\n",
    "print(\"\\nLeft join:\")\n",
    "print(left_join)\n",
    "\n",
    "# Right join\n",
    "right_join = pd.merge(df1, df2, on='ID', how='right')\n",
    "print(\"\\nRight join:\")\n",
    "print(right_join)\n",
    "\n",
    "# Outer join\n",
    "outer_join = pd.merge(df1, df2, on='ID', how='outer')\n",
    "print(\"\\nOuter join:\")\n",
    "print(outer_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca9c266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join on different column names\n",
    "dept_budget = pd.merge(df1, df3, on='Department', how='left')\n",
    "print(\"Joining on Department column:\")\n",
    "print(dept_budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5619315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating DataFrames\n",
    "df4 = pd.DataFrame({\n",
    "    'ID': ['A6', 'A7'],\n",
    "    'Name': ['Mark', 'Lisa'],\n",
    "    'Department': ['Legal', 'HR']\n",
    "})\n",
    "\n",
    "# Vertical concatenation (row-wise)\n",
    "df_concat_rows = pd.concat([df1, df4], ignore_index=True)\n",
    "print(\"Concatenated rows:\")\n",
    "print(df_concat_rows)\n",
    "\n",
    "# Horizontal concatenation (column-wise)\n",
    "df5 = pd.DataFrame({\n",
    "    'Performance_Score': [4.5, 3.9, 4.2, 4.7],\n",
    "    'Bonus': [2000, 1500, 1800, 2200]\n",
    "}, index=['A1', 'A2', 'A3', 'A4'])\n",
    "\n",
    "df1_with_index = df1.copy()\n",
    "df1_with_index.set_index('ID', inplace=True)\n",
    "\n",
    "df_concat_cols = pd.concat([df1_with_index, df5], axis=1)\n",
    "print(\"\\nConcatenated columns:\")\n",
    "print(df_concat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d790aa0",
   "metadata": {},
   "source": [
    "## 8. Working with Time Series Data\n",
    "\n",
    "Pandas has excellent support for time series analysis.\n",
    "\n",
    "### Real-Life Use Case: Energy Consumption Forecasting\n",
    "\n",
    "Utility companies need to forecast energy demand to efficiently manage power generation and distribution. Using pandas time series capabilities, data scientists can analyze historical consumption patterns, identify seasonal trends (e.g., higher usage during summer months for cooling), detect daily patterns (peak hours vs. off-peak), and account for special events or holidays. By resampling hourly data to daily or weekly aggregates, calculating rolling averages to smooth out noise, and extracting time-based features (day of week, month, holiday indicators), they can build accurate forecasting models. These forecasts help optimize power generation schedules, reduce costs, and ensure reliable service during peak demand periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4c5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date column to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Set Date as index\n",
    "ts_df = df.set_index('Date')\n",
    "print(\"Time series data (first few rows):\")\n",
    "print(ts_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ebcc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting date components\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['Weekday'] = df['Date'].dt.day_name()\n",
    "\n",
    "print(\"Date components:\")\n",
    "df[['Date', 'Year', 'Month', 'Day', 'Weekday']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series resampling\n",
    "# Daily to weekly resampling\n",
    "weekly_sales = ts_df.resample('W')['Revenue'].sum()\n",
    "print(\"Weekly sales:\")\n",
    "print(weekly_sales.head())\n",
    "\n",
    "# Daily to monthly resampling\n",
    "monthly_sales = ts_df.resample('M').agg({\n",
    "    'Revenue': 'sum',\n",
    "    'Units_Sold': 'sum',\n",
    "    'Customer_Satisfaction': 'mean'\n",
    "})\n",
    "print(\"\\nMonthly aggregated data:\")\n",
    "print(monthly_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffcd4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based filtering\n",
    "# Filter data for a specific month\n",
    "jan_data = ts_df['2023-01-01':'2023-01-31']\n",
    "print(f\"January data shape: {jan_data.shape}\")\n",
    "\n",
    "# Filter data between two dates\n",
    "q1_data = ts_df['2023-01-01':'2023-03-31']\n",
    "print(f\"Q1 data shape: {q1_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e6c867",
   "metadata": {},
   "source": [
    "## 9. Practical Exercises\n",
    "\n",
    "Let's consolidate our knowledge with some practical exercises.\n",
    "\n",
    "### Real-Life Use Case: Business Intelligence Dashboard\n",
    "\n",
    "A business intelligence team needs to create interactive dashboards for company executives. The exercises below mimic the type of data preparation and analysis required to power these dashboards. By practicing these skills, you'll learn how to transform raw data into meaningful business insights that drive decision-making. In real-world scenarios, the output of these analyses would feed into visualization tools like Tableau, Power BI, or custom web dashboards used by stakeholders across the organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3bf5b5",
   "metadata": {},
   "source": [
    "### Exercise 1: Basic Data Analysis\n",
    "\n",
    "1. Load the sales data\n",
    "2. Find the top 5 days with highest revenue\n",
    "3. Calculate the average satisfaction score by product\n",
    "4. Identify which store has the highest average revenue per transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c2e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solution\n",
    "\n",
    "# 1. Load the sales data\n",
    "sales = pd.read_csv('sales_data.csv')\n",
    "sales['Date'] = pd.to_datetime(sales['Date'])\n",
    "\n",
    "# 2. Find the top 5 days with highest revenue\n",
    "top_revenue_days = sales.groupby('Date')['Revenue'].sum().sort_values(ascending=False).head(5)\n",
    "print(\"Top 5 days with highest revenue:\")\n",
    "print(top_revenue_days)\n",
    "\n",
    "# 3. Calculate the average satisfaction score by product\n",
    "product_satisfaction = sales.groupby('Product')['Customer_Satisfaction'].mean().sort_values(ascending=False)\n",
    "print(\"\\nAverage satisfaction score by product:\")\n",
    "print(product_satisfaction)\n",
    "\n",
    "# 4. Identify which store has the highest average revenue per transaction\n",
    "store_avg_revenue = sales.groupby('Store')['Revenue'].mean().sort_values(ascending=False)\n",
    "print(\"\\nAverage revenue per transaction by store:\")\n",
    "print(store_avg_revenue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11a2759",
   "metadata": {},
   "source": [
    "### Exercise 2: Data Transformation Challenge\n",
    "\n",
    "1. Create a new column that categorizes revenue into 'Low', 'Medium', 'High' based on percentiles\n",
    "2. Calculate a 7-day moving average of revenue\n",
    "3. Find the correlation between units sold and customer satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1321e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 Solution\n",
    "\n",
    "# 1. Create a new column that categorizes revenue into 'Low', 'Medium', 'High' based on percentiles\n",
    "sales['Revenue_Category'] = pd.qcut(\n",
    "    sales['Revenue'], \n",
    "    q=[0, 0.33, 0.67, 1], \n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "print(\"Revenue categories count:\")\n",
    "print(sales['Revenue_Category'].value_counts())\n",
    "\n",
    "# 2. Calculate a 7-day moving average of revenue\n",
    "ts_sales = sales.set_index('Date').sort_index()\n",
    "revenue_ma = ts_sales['Revenue'].rolling(window=7).mean()\n",
    "\n",
    "print(\"\\n7-day moving average of revenue (first 10 days):\")\n",
    "print(revenue_ma.head(10))\n",
    "\n",
    "# 3. Find the correlation between units sold and customer satisfaction\n",
    "correlation = sales[['Units_Sold', 'Customer_Satisfaction', 'Revenue']].corr()\n",
    "print(\"\\nCorrelation matrix:\")\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8446d1",
   "metadata": {},
   "source": [
    "### Exercise 3: Advanced Analysis\n",
    "\n",
    "1. Identify which product-store combination generates the highest total revenue\n",
    "2. Calculate the day of week effect on sales\n",
    "3. Create a pivot table showing total revenue by store and product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d380c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 Solution\n",
    "\n",
    "# 1. Identify which product-store combination generates the highest total revenue\n",
    "product_store_revenue = sales.groupby(['Store', 'Product'])['Revenue'].sum().reset_index()\n",
    "top_combinations = product_store_revenue.sort_values(by='Revenue', ascending=False).head(5)\n",
    "print(\"Top 5 product-store combinations by revenue:\")\n",
    "print(top_combinations)\n",
    "\n",
    "# 2. Calculate the day of week effect on sales\n",
    "sales['Weekday'] = sales['Date'].dt.day_name()\n",
    "weekday_sales = sales.groupby('Weekday')['Revenue'].agg(['sum', 'mean'])\n",
    "# Reorder days of week\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekday_sales = weekday_sales.reindex(weekday_order)\n",
    "print(\"\\nSales by day of week:\")\n",
    "print(weekday_sales)\n",
    "\n",
    "# 3. Create a pivot table showing total revenue by store and product\n",
    "pivot = pd.pivot_table(sales, values='Revenue', index='Store', columns='Product', \n",
    "                       aggfunc='sum', fill_value=0)\n",
    "print(\"\\nTotal revenue by store and product:\")\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d5788c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've covered key pandas functionalities for data manipulation:\n",
    "\n",
    "- **Creating and working with pandas data structures** (Series, DataFrame) - *essential for organizing any data analysis project*\n",
    "- **Reading and writing data from/to various formats** - *critical for working with diverse data sources in business environments*\n",
    "- **Data inspection and cleaning techniques** - *fundamental for ensuring data quality in any analytics workflow*\n",
    "- **Different ways to select, filter, and index data** - *powerful capabilities for focusing on relevant subsets of your data*\n",
    "- **Data transformation and aggregation methods** - *key for summarizing information and extracting insights*\n",
    "- **Handling missing data effectively** - *essential for maintaining data integrity despite incomplete information*\n",
    "- **Combining datasets through merging, joining, and concatenation** - *important for creating comprehensive unified datasets*\n",
    "- **Working with time series data** - *crucial for analyzing patterns over time and making forecasts*\n",
    "- **Practical exercises to apply these concepts** - *reinforcing learning through hands-on problem-solving*\n",
    "\n",
    "These pandas skills form the backbone of modern data analysis in Python, applicable across industries from finance and healthcare to marketing and environmental science. Mastering these techniques will enable you to efficiently transform raw data into actionable insights in virtually any data-driven role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f40cee",
   "metadata": {},
   "source": [
    "## Additional Real-World Case Studies\n",
    "\n",
    "### 1. Fraud Detection in Banking\n",
    "\n",
    "Banks use pandas to analyze transaction data for suspicious patterns. Data scientists create features like transaction frequency, amount deviations from historical patterns, and geographic anomalies using pandas groupby, rolling window functions, and filtering operations. These derived features feed into machine learning models that flag potentially fraudulent transactions for further investigation.\n",
    "\n",
    "```python\n",
    "# Example of transaction frequency analysis\n",
    "hourly_transactions = df.groupby([df['transaction_date'].dt.date, \n",
    "                                df['customer_id'], \n",
    "                                df['transaction_date'].dt.hour])['amount'].count().reset_index()\n",
    "# Flag unusual transaction frequency\n",
    "customer_hourly_stats = hourly_transactions.groupby('customer_id')['amount'].agg(['mean', 'std'])\n",
    "merged_data = hourly_transactions.merge(customer_hourly_stats, on='customer_id')\n",
    "merged_data['z_score'] = (merged_data['amount'] - merged_data['mean']) / merged_data['std']\n",
    "suspicious = merged_data[merged_data['z_score'] > 3]  # Transactions with unusually high frequency\n",
    "```\n",
    "\n",
    "### 2. Product Recommendation Systems\n",
    "\n",
    "E-commerce companies use pandas to analyze purchase history and build recommendation engines. Data scientists use merge operations to combine user profiles with purchase history, pivot tables to create user-item matrices, and groupby operations to identify frequently co-purchased items. These transformations prepare the data for collaborative filtering algorithms that power \"customers who bought this also bought\" recommendations.\n",
    "\n",
    "### 3. Urban Transportation Planning\n",
    "\n",
    "City planners use pandas to analyze public transit data, including bus/train ridership, traffic patterns, and infrastructure usage. Using time series capabilities, they can identify peak travel times, resampling minute-by-minute data to hourly or daily aggregates. With groupby operations, they can analyze differences between weekdays and weekends, or examine transit usage by neighborhood. These insights help optimize bus routes, traffic light timing, and infrastructure investments.\n",
    "\n",
    "### 4. Clinical Trial Data Analysis\n",
    "\n",
    "Pharmaceutical researchers use pandas to analyze clinical trial results, comparing treatment and control groups across multiple metrics. They use pivot tables to restructure patient visit data from long to wide format, filtering operations to handle inclusion/exclusion criteria, and statistical functions to calculate effect sizes and confidence intervals. Pandas' ability to handle missing values is especially important, as patient data often contains gaps due to missed visits or incomplete tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
