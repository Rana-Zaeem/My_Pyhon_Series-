{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1ddfccb",
   "metadata": {},
   "source": [
    "# Data Manipulation with Pandas\n",
    "\n",
    "This notebook covers essential data manipulation techniques using the pandas library in Python. Pandas is one of the most powerful and flexible tools for data analysis and manipulation in Python.\n",
    "\n",
    "## Topics Covered:\n",
    "1. Introduction to pandas and basic data structures\n",
    "2. Reading and writing data\n",
    "3. Data inspection and cleaning\n",
    "4. Filtering, selecting, and indexing data\n",
    "5. Data transformation and aggregation\n",
    "6. Handling missing data\n",
    "7. Merging, joining, and concatenating dataframes\n",
    "8. Working with time series data\n",
    "9. Practical exercises\n",
    "\n",
    "Each section includes real-life use cases to demonstrate practical applications of these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8b739d",
   "metadata": {},
   "source": [
    "## 1. Introduction to Pandas and Basic Data Structures\n",
    "\n",
    "Pandas provides two primary data structures:\n",
    "- **Series**: One-dimensional array-like object containing a sequence of values with associated labels (index)\n",
    "- **DataFrame**: Two-dimensional tabular data structure with labeled axes (rows and columns)\n",
    "\n",
    "**Real-Life Use Case:** Customer Analytics — Use DataFrames to store and analyze customer data, and Series for individual metrics.\n",
    "\n",
    "**What's next:** We'll import pandas and numpy, and set display options for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b296793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc2d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c338f5b",
   "metadata": {},
   "source": [
    "### Creating a Series\n",
    "A Series is a one-dimensional labeled array. Useful for storing a single column of data or a metric.\n",
    "\n",
    "**Real-life use:** Storing a list of customer ages, product prices, or daily sales.\n",
    "\n",
    "**What's next:** We'll create a Series and inspect its index and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9764b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Series with custom index\n",
    "s = pd.Series([10, 20, 30, 40], index=['a', 'b', 'c', 'd'])\n",
    "print(\"Pandas Series:\")\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e0ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the index and values of the Series\n",
    "print(\"\\nIndex:\", s.index)\n",
    "print(\"Values:\", s.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acff92a",
   "metadata": {},
   "source": [
    "### Creating a DataFrame\n",
    "A DataFrame is a two-dimensional table of data, like a spreadsheet.\n",
    "\n",
    "**Real-life use:** Storing customer records, sales transactions, or survey responses.\n",
    "\n",
    "**What's next:** We'll create a DataFrame from a dictionary and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef56e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from a dictionary\n",
    "data = {\n",
    "    'Name': ['John', 'Anna', 'Peter', 'Linda'],\n",
    "    'Age': [28, 34, 29, 42],\n",
    "    'City': ['New York', 'Paris', 'Berlin', 'London'],\n",
    "    'Salary': [65000, 70000, 62000, 85000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"DataFrame:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f870b2",
   "metadata": {},
   "source": [
    "## 2. Reading and Writing Data\n",
    "\n",
    "Pandas can read and write data from/to various file formats including CSV, Excel, SQL, and JSON.\n",
    "\n",
    "**Real-life use:** Importing sales data from CSV, exporting cleaned data to Excel, or loading JSON from an API.\n",
    "\n",
    "**What's next:** We'll create a sample DataFrame and save it to a CSV file, then read it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e306264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame to save\n",
    "sample_df = pd.DataFrame({\n",
    "    'A': np.random.rand(5),          # Random float values\n",
    "    'B': np.random.randint(0, 10, 5), # Random integers between 0-9\n",
    "    'C': ['foo', 'bar', 'baz', 'qux', 'quux'],  # Text values\n",
    "    'D': pd.date_range('2023-01-01', periods=5)  # Date range\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "sample_df.to_csv('sample_data.csv', index=False)\n",
    "print(\"Data saved to CSV file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef5bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data back from the CSV file\n",
    "df_from_csv = pd.read_csv('sample_data.csv')\n",
    "print(\"Data loaded from CSV:\")\n",
    "df_from_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ad22b9",
   "metadata": {},
   "source": [
    "### Creating a Realistic Sample Dataset\n",
    "Let's create a larger, more realistic dataset for use in later examples.\n",
    "\n",
    "**Real-life use:** Simulating sales data for a retail business.\n",
    "\n",
    "**What's next:** We'll generate a DataFrame with random sales data and save it for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff925684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2023-01-01', periods=100)\n",
    "\n",
    "sales_data = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Store': np.random.choice(['A', 'B', 'C', 'D'], 100),\n",
    "    'Product': np.random.choice(['Widget', 'Gadget', 'Tool', 'Device'], 100),\n",
    "    'Units_Sold': np.random.randint(1, 50, 100),\n",
    "    'Revenue': np.random.randint(100, 5000, 100),\n",
    "    'Customer_Satisfaction': np.random.randint(1, 6, 100)\n",
    "})\n",
    "\n",
    "# Save this dataset for future use\n",
    "sales_data.to_csv('sales_data.csv', index=False)\n",
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bfa40a",
   "metadata": {},
   "source": [
    "## 3. Data Inspection and Cleaning\n",
    "\n",
    "Before analyzing data, it's important to inspect and clean it by checking its structure, identifying missing values, and handling duplicates.\n",
    "\n",
    "**Real-Life Use Case:** Healthcare Data Management — Ensuring patient data accuracy by cleaning electronic health records.\n",
    "\n",
    "**What's next:** We'll load the sales data and explore its structure and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10361be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sales data\n",
    "df = pd.read_csv('sales_data.csv')\n",
    "\n",
    "# Basic information about the DataFrame\n",
    "print(\"Shape (rows, columns):\", df.shape)\n",
    "print(\"\\nColumn names:\", df.columns.tolist())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd65068",
   "metadata": {},
   "source": [
    "### Checking for Missing Values\n",
    "Identifying missing values is crucial for data cleaning.\n",
    "\n",
    "**Real-life use:** Detecting missing patient records, incomplete survey responses, or gaps in time series data.\n",
    "\n",
    "**What's next:** We'll check the sales data for missing values and introduce some for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb745ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's introduce some missing values for demonstration\n",
    "df_with_missing = df.copy()\n",
    "df_with_missing.loc[np.random.choice(df.index, 10), 'Revenue'] = np.nan\n",
    "df_with_missing.loc[np.random.choice(df.index, 5), 'Customer_Satisfaction'] = np.nan\n",
    "\n",
    "print(\"\\nMissing values in the modified dataset:\")\n",
    "print(df_with_missing.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d14c65",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "There are various strategies to handle missing data, including imputation and deletion.\n",
    "\n",
    "**Real-life use:** Filling missing lab results with the average value, or removing duplicate patient records.\n",
    "\n",
    "**What's next:** We'll explore different methods to handle missing values in the sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c25bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "# Method 1: Fill missing values with mean/median/mode\n",
    "df_filled = df_with_missing.copy()\n",
    "df_filled['Revenue'] = df_filled['Revenue'].fillna(df_filled['Revenue'].mean())\n",
    "df_filled['Customer_Satisfaction'] = df_filled['Customer_Satisfaction'].fillna(\n",
    "    df_filled['Customer_Satisfaction'].median())\n",
    "\n",
    "# Method 2: Drop rows with missing values\n",
    "df_dropped = df_with_missing.dropna()\n",
    "\n",
    "print(f\"Original shape: {df_with_missing.shape}\")\n",
    "print(f\"After filling: {df_filled.shape}\")\n",
    "print(f\"After dropping: {df_dropped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1edf82a",
   "metadata": {},
   "source": [
    "### Detecting and Handling Duplicates\n",
    "Duplicates can skew your analysis and should be handled appropriately.\n",
    "\n",
    "**Real-life use:** Removing duplicate entries in a customer database or transaction log.\n",
    "\n",
    "**What's next:** We'll check the sales data for duplicates and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c3ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting and handling duplicates\n",
    "# Let's introduce some duplicates\n",
    "df_with_duplicates = pd.concat([df, df.iloc[:5]], ignore_index=True)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_count = df_with_duplicates.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Remove duplicates\n",
    "df_unique = df_with_duplicates.drop_duplicates()\n",
    "print(f\"Shape after removing duplicates: {df_unique.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5454d614",
   "metadata": {},
   "source": [
    "## 4. Filtering, Selecting, and Indexing Data\n",
    "\n",
    "Pandas provides powerful ways to select, filter, and access data within DataFrames.\n",
    "\n",
    "**Real-Life Use Case:** Marketing Campaign Analysis — Segmenting customers for targeted campaigns.\n",
    "\n",
    "**What's next:** We'll explore different methods to select and filter data in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e27b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic column selection\n",
    "print(\"Selecting a single column as Series:\")\n",
    "print(df['Revenue'].head())\n",
    "\n",
    "print(\"\\nSelecting multiple columns as DataFrame:\")\n",
    "print(df[['Store', 'Product', 'Revenue']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e5bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row selection using iloc (position-based) and loc (label-based)\n",
    "print(\"First 5 rows using iloc:\")\n",
    "print(df.iloc[:5])\n",
    "\n",
    "print(\"\\nRows 10-15:\")\n",
    "print(df.iloc[10:16])\n",
    "\n",
    "print(\"\\nSpecific rows and columns using iloc:\")\n",
    "print(df.iloc[0:3, [1, 3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea24f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering data based on conditions\n",
    "# Filter stores with high revenue\n",
    "high_revenue = df[df['Revenue'] > 4000]\n",
    "print(\"High revenue transactions:\")\n",
    "print(high_revenue)\n",
    "\n",
    "# Filter specific store and product combinations\n",
    "store_a_widgets = df[(df['Store'] == 'A') & (df['Product'] == 'Widget')]\n",
    "print(\"\\nStore A Widget sales:\")\n",
    "print(store_a_widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e555e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using query method for more readable filtering\n",
    "high_satisfaction = df.query('Customer_Satisfaction >= 4 & Units_Sold > 30')\n",
    "print(\"High satisfaction and high volume sales:\")\n",
    "print(high_satisfaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed51bc",
   "metadata": {},
   "source": [
    "## 5. Data Transformation and Aggregation\n",
    "\n",
    "Pandas provides various methods for transforming, grouping, and aggregating data.\n",
    "\n",
    "**Real-Life Use Case:** Retail Sales Analysis — Summarizing sales performance across stores and products.\n",
    "\n",
    "**What's next:** We'll learn how to add new columns, group data, and create pivot tables in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd66dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new columns\n",
    "df['Revenue_per_Unit'] = df['Revenue'] / df['Units_Sold']\n",
    "df['Is_High_Value'] = df['Revenue'] > df['Revenue'].median()\n",
    "\n",
    "# Apply a custom function to a column\n",
    "def categorize_satisfaction(score):\n",
    "    if score >= 4:\n",
    "        return 'High'\n",
    "    elif score >= 2:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "df['Satisfaction_Category'] = df['Customer_Satisfaction'].apply(categorize_satisfaction)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df9ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by operations\n",
    "store_summary = df.groupby('Store').agg({\n",
    "    'Revenue': ['sum', 'mean'], \n",
    "    'Units_Sold': 'sum',\n",
    "    'Customer_Satisfaction': 'mean'\n",
    "})\n",
    "\n",
    "print(\"Store summary:\")\n",
    "store_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6383f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-level groupby\n",
    "product_store_summary = df.groupby(['Store', 'Product']).agg({\n",
    "    'Revenue': 'sum',\n",
    "    'Units_Sold': 'sum',\n",
    "    'Customer_Satisfaction': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "print(\"Product and store summary:\")\n",
    "product_store_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ffd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot tables\n",
    "pivot_table = df.pivot_table(\n",
    "    values=['Revenue', 'Units_Sold'],\n",
    "    index='Store',\n",
    "    columns='Product',\n",
    "    aggfunc={'Revenue': 'sum', 'Units_Sold': 'sum'}\n",
    ")\n",
    "\n",
    "print(\"Pivot table of revenue and units sold by store and product:\")\n",
    "pivot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eece22b9",
   "metadata": {},
   "source": [
    "## 6. Handling Missing Data\n",
    "\n",
    "Let's explore more advanced techniques for handling missing data.\n",
    "\n",
    "**Real-Life Use Case:** Environmental Sensor Data — Handling gaps in sensor data for accurate modeling.\n",
    "\n",
    "**What's next:** We'll learn different imputation methods and how to interpolate missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cd491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with various missing patterns\n",
    "df_missing = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [np.nan, 2, 3, 4, 5],\n",
    "    'C': [1, 2, 3, np.nan, np.nan],\n",
    "    'D': [1, 2, 3, 4, 5]\n",
    "})\n",
    "\n",
    "print(\"Original dataset with missing values:\")\n",
    "print(df_missing)\n",
    "print(\"\\nMissing value count by column:\")\n",
    "print(df_missing.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe33b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different imputation strategies\n",
    "\n",
    "# 1. Fill with a constant value\n",
    "df_fill_const = df_missing.fillna(0)\n",
    "print(\"Filled with constant value:\")\n",
    "print(df_fill_const)\n",
    "\n",
    "# 2. Fill with different values for each column\n",
    "df_fill_dict = df_missing.fillna({'A': 0, 'B': 10, 'C': -1})\n",
    "print(\"\\nFilled with different values per column:\")\n",
    "print(df_fill_dict)\n",
    "\n",
    "# 3. Forward fill (propagate last valid observation forward)\n",
    "df_ffill = df_missing.fillna(method='ffill')\n",
    "print(\"\\nForward fill:\")\n",
    "print(df_ffill)\n",
    "\n",
    "# 4. Backward fill (use next valid observation to fill gap)\n",
    "df_bfill = df_missing.fillna(method='bfill')\n",
    "print(\"\\nBackward fill:\")\n",
    "print(df_bfill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffcae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation methods\n",
    "df_interp = df_missing.interpolate(method='linear')\n",
    "print(\"Linear interpolation:\")\n",
    "print(df_interp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098a4e73",
   "metadata": {},
   "source": [
    "## 7. Merging, Joining, and Concatenating DataFrames\n",
    "\n",
    "Pandas provides various ways to combine multiple DataFrames together.\n",
    "\n",
    "**Real-Life Use Case:** Supply Chain Management — Combining data from different systems for end-to-end visibility.\n",
    "\n",
    "**What's next:** We'll learn how to merge, join, and concatenate DataFrames in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9f3469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrames\n",
    "df1 = pd.DataFrame({\n",
    "    'ID': ['A1', 'A2', 'A3', 'A4'],\n",
    "    'Name': ['John', 'Emily', 'Martha', 'Samuel'],\n",
    "    'Department': ['HR', 'Marketing', 'Finance', 'IT']\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'ID': ['A2', 'A3', 'A4', 'A5'],\n",
    "    'Salary': [60000, 80000, 70000, 90000],\n",
    "    'Years_Employed': [3, 7, 4, 2]\n",
    "})\n",
    "\n",
    "df3 = pd.DataFrame({\n",
    "    'Department': ['HR', 'Marketing', 'Finance', 'IT', 'Operations'],\n",
    "    'Budget': [100000, 200000, 300000, 250000, 150000]\n",
    "})\n",
    "\n",
    "print(\"DataFrame 1:\")\n",
    "print(df1)\n",
    "print(\"\\nDataFrame 2:\")\n",
    "print(df2)\n",
    "print(\"\\nDataFrame 3:\")\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a7ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different join types\n",
    "\n",
    "# Inner join\n",
    "inner_join = pd.merge(df1, df2, on='ID', how='inner')\n",
    "print(\"Inner join:\")\n",
    "print(inner_join)\n",
    "\n",
    "# Left join\n",
    "left_join = pd.merge(df1, df2, on='ID', how='left')\n",
    "print(\"\\nLeft join:\")\n",
    "print(left_join)\n",
    "\n",
    "# Right join\n",
    "right_join = pd.merge(df1, df2, on='ID', how='right')\n",
    "print(\"\\nRight join:\")\n",
    "print(right_join)\n",
    "\n",
    "# Outer join\n",
    "outer_join = pd.merge(df1, df2, on='ID', how='outer')\n",
    "print(\"\\nOuter join:\")\n",
    "print(outer_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8bb605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join on different column names\n",
    "dept_budget = pd.merge(df1, df3, on='Department', how='left')\n",
    "print(\"Joining on Department column:\")\n",
    "print(dept_budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating DataFrames\n",
    "df4 = pd.DataFrame({\n",
    "    'ID': ['A6', 'A7'],\n",
    "    'Name': ['Mark', 'Lisa'],\n",
    "    'Department': ['Legal', 'HR']\n",
    "})\n",
    "\n",
    "# Vertical concatenation (row-wise)\n",
    "df_concat_rows = pd.concat([df1, df4], ignore_index=True)\n",
    "print(\"Concatenated rows:\")\n",
    "print(df_concat_rows)\n",
    "\n",
    "# Horizontal concatenation (column-wise)\n",
    "df5 = pd.DataFrame({\n",
    "    'Performance_Score': [4.5, 3.9, 4.2, 4.7],\n",
    "    'Bonus': [2000, 1500, 1800, 2200]\n",
    "}, index=['A1', 'A2', 'A3', 'A4'])\n",
    "\n",
    "df1_with_index = df1.copy()\n",
    "df1_with_index.set_index('ID', inplace=True)\n",
    "\n",
    "df_concat_cols = pd.concat([df1_with_index, df5], axis=1)\n",
    "print(\"\\nConcatenated columns:\")\n",
    "print(df_concat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025eb32f",
   "metadata": {},
   "source": [
    "## 8. Working with Time Series Data\n",
    "\n",
    "Pandas has excellent support for time series analysis.\n",
    "\n",
    "**Real-Life Use Case:** Energy Consumption Forecasting — Analyzing and forecasting energy demand.\n",
    "\n",
    "**What's next:** We'll learn how to work with date and time data, and perform time series analysis in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date column to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Set Date as index\n",
    "ts_df = df.set_index('Date')\n",
    "print(\"Time series data (first few rows):\")\n",
    "print(ts_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f532879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting date components\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['Weekday'] = df['Date'].dt.day_name()\n",
    "\n",
    "print(\"Date components:\")\n",
    "df[['Date', 'Year', 'Month', 'Day', 'Weekday']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6028322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series resampling\n",
    "# Daily to weekly resampling\n",
    "weekly_sales = ts_df.resample('W')['Revenue'].sum()\n",
    "print(\"Weekly sales:\")\n",
    "print(weekly_sales.head())\n",
    "\n",
    "# Daily to monthly resampling\n",
    "monthly_sales = ts_df.resample('M').agg({\n",
    "    'Revenue': 'sum',\n",
    "    'Units_Sold': 'sum',\n",
    "    'Customer_Satisfaction': 'mean'\n",
    "})\n",
    "print(\"\\nMonthly aggregated data:\")\n",
    "print(monthly_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based filtering\n",
    "# Filter data for a specific month\n",
    "jan_data = ts_df['2023-01-01':'2023-01-31']\n",
    "print(f\"January data shape: {jan_data.shape}\")\n",
    "\n",
    "# Filter data between two dates\n",
    "q1_data = ts_df['2023-01-01':'2023-03-31']\n",
    "print(f\"Q1 data shape: {q1_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e29870d",
   "metadata": {},
   "source": [
    "## 9. Practical Exercises\n",
    "\n",
    "Let's consolidate our knowledge with some practical exercises.\n",
    "\n",
    "**Real-Life Use Case:** Business Intelligence Dashboard — Preparing data for interactive executive dashboards.\n",
    "\n",
    "**What's next:** Complete the exercises to reinforce your learning and apply pandas skills to real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa87d49d",
   "metadata": {},
   "source": [
    "### Exercise 1: Basic Data Analysis\n",
    "\n",
    "1. Load the sales data\n",
    "2. Find the top 5 days with highest revenue\n",
    "3. Calculate the average satisfaction score by product\n",
    "4. Identify which store has the highest average revenue per transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b53e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solution\n",
    "\n",
    "# 1. Load the sales data\n",
    "sales = pd.read_csv('sales_data.csv')\n",
    "sales['Date'] = pd.to_datetime(sales['Date'])\n",
    "\n",
    "# 2. Find the top 5 days with highest revenue\n",
    "top_revenue_days = sales.groupby('Date')['Revenue'].sum().sort_values(ascending=False).head(5)\n",
    "print(\"Top 5 days with highest revenue:\")\n",
    "print(top_revenue_days)\n",
    "\n",
    "# 3. Calculate the average satisfaction score by product\n",
    "product_satisfaction = sales.groupby('Product')['Customer_Satisfaction'].mean().sort_values(ascending=False)\n",
    "print(\"\\nAverage satisfaction score by product:\")\n",
    "print(product_satisfaction)\n",
    "\n",
    "# 4. Identify which store has the highest average revenue per transaction\n",
    "store_avg_revenue = sales.groupby('Store')['Revenue'].mean().sort_values(ascending=False)\n",
    "print(\"\\nAverage revenue per transaction by store:\")\n",
    "print(store_avg_revenue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d822861",
   "metadata": {},
   "source": [
    "### Exercise 2: Data Transformation Challenge\n",
    "\n",
    "1. Create a new column that categorizes revenue into 'Low', 'Medium', 'High' based on percentiles\n",
    "2. Calculate a 7-day moving average of revenue\n",
    "3. Find the correlation between units sold and customer satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb11a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 Solution\n",
    "\n",
    "# 1. Create a new column that categorizes revenue into 'Low', 'Medium', 'High' based on percentiles\n",
    "sales['Revenue_Category'] = pd.qcut(\n",
    "    sales['Revenue'], \n",
    "    q=[0, 0.33, 0.67, 1], \n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "print(\"Revenue categories count:\")\n",
    "print(sales['Revenue_Category'].value_counts())\n",
    "\n",
    "# 2. Calculate a 7-day moving average of revenue\n",
    "ts_sales = sales.set_index('Date').sort_index()\n",
    "revenue_ma = ts_sales['Revenue'].rolling(window=7).mean()\n",
    "\n",
    "print(\"\\n7-day moving average of revenue (first 10 days):\")\n",
    "print(revenue_ma.head(10))\n",
    "\n",
    "# 3. Find the correlation between units sold and customer satisfaction\n",
    "correlation = sales[['Units_Sold', 'Customer_Satisfaction', 'Revenue']].corr()\n",
    "print(\"\\nCorrelation matrix:\")\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc6c580",
   "metadata": {},
   "source": [
    "### Exercise 3: Advanced Analysis\n",
    "\n",
    "1. Identify which product-store combination generates the highest total revenue\n",
    "2. Calculate the day of week effect on sales\n",
    "3. Create a pivot table showing total revenue by store and product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e52f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 Solution\n",
    "\n",
    "# 1. Identify which product-store combination generates the highest total revenue\n",
    "product_store_revenue = sales.groupby(['Store', 'Product'])['Revenue'].sum().reset_index()\n",
    "top_combinations = product_store_revenue.sort_values(by='Revenue', ascending=False).head(5)\n",
    "print(\"Top 5 product-store combinations by revenue:\")\n",
    "print(top_combinations)\n",
    "\n",
    "# 2. Calculate the day of week effect on sales\n",
    "sales['Weekday'] = sales['Date'].dt.day_name()\n",
    "weekday_sales = sales.groupby('Weekday')['Revenue'].agg(['sum', 'mean'])\n",
    "# Reorder days of week\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekday_sales = weekday_sales.reindex(weekday_order)\n",
    "print(\"\\nSales by day of week:\")\n",
    "print(weekday_sales)\n",
    "\n",
    "# 3. Create a pivot table showing total revenue by store and product\n",
    "pivot = pd.pivot_table(sales, values='Revenue', index='Store', columns='Product', \n",
    "                       aggfunc='sum', fill_value=0)\n",
    "print(\"\\nTotal revenue by store and product:\")\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d411a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've covered key pandas functionalities for data manipulation:\n",
    "\n",
    "- **Creating and working with pandas data structures** (Series, DataFrame) - *essential for organizing any data analysis project*\n",
    "- **Reading and writing data from/to various formats** - *critical for working with diverse data sources in business environments*\n",
    "- **Data inspection and cleaning techniques** - *fundamental for ensuring data quality in any analytics workflow*\n",
    "- **Different ways to select, filter, and index data** - *powerful capabilities for focusing on relevant subsets of your data*\n",
    "- **Data transformation and aggregation methods** - *key for summarizing information and extracting insights*\n",
    "- **Handling missing data effectively** - *essential for maintaining data integrity despite incomplete information*\n",
    "- **Combining datasets through merging, joining, and concatenation** - *important for creating comprehensive unified datasets*\n",
    "- **Working with time series data** - *crucial for analyzing patterns over time and making forecasts*\n",
    "- **Practical exercises to apply these concepts** - *reinforcing learning through hands-on problem-solving*\n",
    "\n",
    "These pandas skills form the backbone of modern data analysis in Python, applicable across industries from finance and healthcare to marketing and environmental science. Mastering these techniques will enable you to efficiently transform raw data into actionable insights in virtually any data-driven role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a7c553",
   "metadata": {},
   "source": [
    "## Additional Real-World Case Studies\n",
    "\n",
    "### 1. Fraud Detection in Banking\n",
    "\n",
    "Banks use pandas to analyze transaction data for suspicious patterns. Data scientists create features like transaction frequency, amount deviations from historical patterns, and geographic anomalies using pandas groupby, rolling window functions, and filtering operations. These derived features feed into machine learning models that flag potentially fraudulent transactions for further investigation.\n",
    "\n",
    "```python\n",
    "# Example of transaction frequency analysis\n",
    "hourly_transactions = df.groupby([df['transaction_date'].dt.date, \n",
    "                                df['customer_id'], \n",
    "                                df['transaction_date'].dt.hour])['amount'].count().reset_index()\n",
    "# Flag unusual transaction frequency\n",
    "customer_hourly_stats = hourly_transactions.groupby('customer_id')['amount'].agg(['mean', 'std'])\n",
    "merged_data = hourly_transactions.merge(customer_hourly_stats, on='customer_id')\n",
    "merged_data['z_score'] = (merged_data['amount'] - merged_data['mean']) / merged_data['std']\n",
    "suspicious = merged_data[merged_data['z_score'] > 3]  # Transactions with unusually high frequency\n",
    "```\n",
    "\n",
    "### 2. Product Recommendation Systems\n",
    "\n",
    "E-commerce companies use pandas to analyze purchase history and build recommendation engines. Data scientists use merge operations to combine user profiles with purchase history, pivot tables to create user-item matrices, and groupby operations to identify frequently co-purchased items. These transformations prepare the data for collaborative filtering algorithms that power \"customers who bought this also bought\" recommendations.\n",
    "\n",
    "### 3. Urban Transportation Planning\n",
    "\n",
    "City planners use pandas to analyze public transit data, including bus/train ridership, traffic patterns, and infrastructure usage. Using time series capabilities, they can identify peak travel times, resampling minute-by-minute data to hourly or daily aggregates. With groupby operations, they can analyze differences between weekdays and weekends, or examine transit usage by neighborhood. These insights help optimize bus routes, traffic light timing, and infrastructure investments.\n",
    "\n",
    "### 4. Clinical Trial Data Analysis\n",
    "\n",
    "Pharmaceutical researchers use pandas to analyze clinical trial results, comparing treatment and control groups across multiple metrics. They use pivot tables to restructure patient visit data from long to wide format, filtering operations to handle inclusion/exclusion criteria, and statistical functions to calculate effect sizes and confidence intervals. Pandas' ability to handle missing values is especially important, as patient data often contains gaps due to missed visits or incomplete tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
