{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc55489",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning and AI in Python\n",
    "\n",
    "This notebook covers advanced machine learning and AI concepts using Python. You'll learn about ensemble methods, model selection, deep learning basics, natural language processing (NLP), and model deployment.\n",
    "\n",
    "## Topics Covered:\n",
    "1. Ensemble Methods (Bagging, Boosting, Random Forest, Gradient Boosting)\n",
    "2. Model Selection and Hyperparameter Tuning\n",
    "3. Introduction to Deep Learning (Neural Networks)\n",
    "4. Natural Language Processing (NLP) Basics\n",
    "5. Model Deployment and Serving\n",
    "6. Real-Life Use Cases and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75722954",
   "metadata": {},
   "source": [
    "## 1. Ensemble Methods\n",
    "\n",
    "Ensemble methods combine multiple models to improve performance and robustness. Popular techniques include bagging (e.g., Random Forest) and boosting (e.g., Gradient Boosting, AdaBoost).\n",
    "\n",
    "**Real-life use case:** Credit card fraud detection systems use ensemble models to reduce false positives and improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2c38c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (142, 13), Test data shape: (36, 13)\n",
      "Random Forest Accuracy: 1.0\n",
      "Random Forest Accuracy: 1.0\n",
      "Gradient Boosting Accuracy: 0.9444444444444444\n",
      "Gradient Boosting Accuracy: 0.9444444444444444\n",
      "AdaBoost Accuracy: 0.9444444444444444\n",
      "\n",
      "Best model: Random Forest with accuracy: 1.0000\n",
      "\n",
      "Classification report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n",
      "AdaBoost Accuracy: 0.9444444444444444\n",
      "\n",
      "Best model: Random Forest with accuracy: 1.0000\n",
      "\n",
      "Classification report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine(as_frame=True)  # Load the wine dataset with Pandas DataFrame\n",
    "X, y = wine.data, wine.target  # Features and target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training data shape: {X_train.shape}, Test data shape: {X_test.shape}\")\n",
    "\n",
    "# Random Forest - An ensemble of decision trees using bagging\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)  # 100 trees in the forest\n",
    "rf.fit(X_train, y_train)  # Train the random forest\n",
    "rf_pred = rf.predict(X_test)  # Make predictions\n",
    "print('Random Forest Accuracy:', accuracy_score(y_test, rf_pred))\n",
    "\n",
    "# Gradient Boosting - Trees are built sequentially to correct errors\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)  # 100 boosting stages\n",
    "gb.fit(X_train, y_train)  # Train the gradient boosting model\n",
    "gb_pred = gb.predict(X_test)  # Make predictions\n",
    "print('Gradient Boosting Accuracy:', accuracy_score(y_test, gb_pred))\n",
    "\n",
    "# AdaBoost - Adaptive Boosting that focuses on misclassified samples\n",
    "ada = AdaBoostClassifier(n_estimators=100, random_state=42)  # 100 boosting stages\n",
    "ada.fit(X_train, y_train)  # Train the AdaBoost model\n",
    "ada_pred = ada.predict(X_test)  # Make predictions\n",
    "print('AdaBoost Accuracy:', accuracy_score(y_test, ada_pred))\n",
    "\n",
    "# Find the best model and print detailed metrics\n",
    "accuracies = [accuracy_score(y_test, rf_pred), \n",
    "             accuracy_score(y_test, gb_pred), \n",
    "             accuracy_score(y_test, ada_pred)]\n",
    "best_idx = np.argmax(accuracies)\n",
    "model_names = ['Random Forest', 'Gradient Boosting', 'AdaBoost']\n",
    "best_model = model_names[best_idx]\n",
    "\n",
    "print(f\"\\nBest model: {best_model} with accuracy: {accuracies[best_idx]:.4f}\")\n",
    "print(f\"\\nClassification report for {best_model}:\")\n",
    "if best_idx == 0:\n",
    "    print(classification_report(y_test, rf_pred))\n",
    "elif best_idx == 1:\n",
    "    print(classification_report(y_test, gb_pred))\n",
    "else:\n",
    "    print(classification_report(y_test, ada_pred))\n",
    "\n",
    "# Output (example):\n",
    "# Training data shape: (142, 13), Test data shape: (36, 13)\n",
    "# Random Forest Accuracy: 0.9722222222222222\n",
    "# Gradient Boosting Accuracy: 0.9722222222222222\n",
    "# AdaBoost Accuracy: 0.9444444444444444\n",
    "#\n",
    "# Best model: Random Forest with accuracy: 0.9722\n",
    "#\n",
    "# Classification report for Random Forest:\n",
    "#               precision    recall  f1-score   support\n",
    "#            0       1.00      1.00      1.00        14\n",
    "#            1       0.93      1.00      0.96        13\n",
    "#            2       1.00      0.89      0.94         9\n",
    "#     accuracy                           0.97        36\n",
    "#    macro avg       0.98      0.96      0.97        36\n",
    "# weighted avg       0.97      0.97      0.97        36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c70c6a4",
   "metadata": {},
   "source": [
    "## 2. Model Selection and Hyperparameter Tuning\n",
    "\n",
    "Selecting the best model and tuning its hyperparameters is crucial for optimal performance. Techniques include cross-validation and grid/randomized search.\n",
    "\n",
    "**Real-life use case:** Tuning a model for predicting loan defaults to maximize recall and minimize false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "895f47b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'n_estimators': 100}\n",
      "Best cross-validated score: 0.9858156028368793\n",
      "Cross-validation scores: [0.97222222 0.94444444 0.97222222 0.97142857 1.        ]\n",
      "Mean CV score: 0.9720634920634922\n",
      "Cross-validation scores: [0.97222222 0.94444444 0.97222222 0.97142857 1.        ]\n",
      "Mean CV score: 0.9720634920634922\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Grid search for Random Forest hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],  # Number of trees in the forest\n",
    "    'max_depth': [None, 5, 10]   # Maximum depth of each tree\n",
    "}\n",
    "\n",
    "# Perform grid search with 3-fold cross-validation\n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)  # Find the best parameters\n",
    "\n",
    "# Print the results\n",
    "print('Best parameters:', grid.best_params_)\n",
    "print('Best cross-validated score:', grid.best_score_)\n",
    "\n",
    "# Use the best parameters found to evaluate on fresh cross-validation folds\n",
    "cv_scores = cross_val_score(RandomForestClassifier(**grid.best_params_, random_state=42), \n",
    "                           X, y, cv=5)  # 5-fold cross-validation\n",
    "print('Cross-validation scores:', cv_scores)\n",
    "print('Mean CV score:', cv_scores.mean())\n",
    "\n",
    "# Output (example):\n",
    "# Best parameters: {'max_depth': None, 'n_estimators': 100}\n",
    "# Best cross-validated score: 0.9647887323943662\n",
    "# Cross-validation scores: [0.94444444 0.94444444 1.         0.97142857 0.94285714]\n",
    "# Mean CV score: 0.9606349206349206\n",
    "#\n",
    "# Interpretation:\n",
    "# - The grid search tested 6 combinations of hyperparameters (2 n_estimators × 3 max_depth options)\n",
    "# - The best model uses 100 trees with unlimited depth\n",
    "# - 5-fold cross-validation shows consistently high performance across different data splits\n",
    "# - Average accuracy is about 96%, which is very good for this wine classification task\n",
    "# - We now have a tuned model that will likely generalize well to new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221339e4",
   "metadata": {},
   "source": [
    "## 3. Introduction to Deep Learning (Neural Networks)\n",
    "\n",
    "Deep learning uses neural networks with multiple layers to learn complex patterns. Keras (with TensorFlow backend) is a popular library for building neural networks in Python.\n",
    "\n",
    "**Real-life use case:** Image recognition in self-driving cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb20de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Prepare data for neural network - convert target to one-hot encoding\n",
    "lb = LabelBinarizer()  # Initialize the label binarizer\n",
    "y_train_bin = lb.fit_transform(y_train)  # Transform training labels to one-hot encoding\n",
    "y_test_bin = lb.transform(y_test)  # Transform test labels using the same encoding\n",
    "\n",
    "# Print the shape of the transformed targets\n",
    "print(f\"Original y_train shape: {y_train.shape}, One-hot encoded shape: {y_train_bin.shape}\")\n",
    "\n",
    "# Build a simple neural network with two hidden layers\n",
    "model = keras.Sequential([\n",
    "    # Input layer with 32 neurons and ReLU activation\n",
    "    keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    # Hidden layer with 16 neurons and ReLU activation\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    # Output layer with softmax activation (one neuron per class)\n",
    "    keras.layers.Dense(len(lb.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()\n",
    "\n",
    "# Compile the model with appropriate loss function and optimizer\n",
    "model.compile(optimizer='adam',  # Adam optimizer - adaptive learning rate\n",
    "              loss='categorical_crossentropy',  # Cross entropy loss for multi-class classification\n",
    "              metrics=['accuracy'])  # Track accuracy during training\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_bin,  # Training data\n",
    "          epochs=20,  # Number of complete passes through the dataset\n",
    "          batch_size=8,  # Number of samples per gradient update\n",
    "          verbose=0)  # Don't print progress\n",
    "\n",
    "# Evaluate on test data\n",
    "loss, acc = model.evaluate(X_test, y_test_bin, verbose=0)\n",
    "print('Neural Network Test Accuracy:', acc)\n",
    "\n",
    "# Output (example):\n",
    "# Original y_train shape: (142,), One-hot encoded shape: (142, 3)\n",
    "# Model: \"sequential\"\n",
    "# _________________________________________________________________\n",
    "#  Layer (type)                Output Shape              Param #   \n",
    "# =================================================================\n",
    "#  dense (Dense)               (None, 32)                448       \n",
    "#                                                                 \n",
    "#  dense_1 (Dense)             (None, 16)                528       \n",
    "#                                                                 \n",
    "#  dense_2 (Dense)             (None, 3)                 51        \n",
    "#                                                                 \n",
    "# =================================================================\n",
    "# Total params: 1027 (4.01 KB)\n",
    "# Trainable params: 1027 (4.01 KB)\n",
    "# Non-trainable params: 0 (0.00 Byte)\n",
    "# _________________________________________________________________\n",
    "#\n",
    "# Neural Network Test Accuracy: 0.9722222089767456\n",
    "#\n",
    "# Interpretation:\n",
    "# - Our neural network has 3 layers with a total of 1,027 trainable parameters\n",
    "# - The model achieved ~97% accuracy on the test set without much tuning\n",
    "# - This demonstrates that even simple neural networks can perform well on structured data\n",
    "# - For complex tasks (images, text, etc.), deeper networks would be necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e7f6c",
   "metadata": {},
   "source": [
    "## 4. Natural Language Processing (NLP) Basics\n",
    "\n",
    "NLP enables computers to understand and process human language. Common tasks include text classification, sentiment analysis, and topic modeling.\n",
    "\n",
    "**Real-life use case:** Sentiment analysis of customer reviews for brand monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3079920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Example text data - simple sentiment analysis task\n",
    "texts = ['I love this product', 'Worst experience ever', 'Amazing quality', \n",
    "        'Not good', 'Will buy again', 'Terrible support']\n",
    "labels = [1, 0, 1, 0, 1, 0]  # 1=positive, 0=negative\n",
    "\n",
    "# Convert text to numerical features (bag of words)\n",
    "vectorizer = CountVectorizer()  # Initialize the vectorizer\n",
    "X_text = vectorizer.fit_transform(texts)  # Transform text to word count matrix\n",
    "\n",
    "# Show the feature names (words)\n",
    "print(\"Feature names (vocabulary):\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Show the document-term matrix shape\n",
    "print(\"Document-term matrix shape:\", X_text.shape)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "clf = MultinomialNB()  # Initialize the classifier\n",
    "clf.fit(X_text, labels)  # Train on our labeled data\n",
    "pred = clf.predict(X_text)  # Predict on the same data\n",
    "print('Text Classification Accuracy:', accuracy_score(labels, pred))\n",
    "\n",
    "# Test on new examples\n",
    "new_texts = ['This is excellent', 'I am disappointed']\n",
    "new_X = vectorizer.transform(new_texts)  # Transform using the same vocabulary\n",
    "new_pred = clf.predict(new_X)  # Predict sentiment\n",
    "print('\\nPredictions for new texts:')\n",
    "for text, sentiment in zip(new_texts, new_pred):\n",
    "    sentiment_label = 'Positive' if sentiment == 1 else 'Negative'\n",
    "    print(f'\"{text}\" -> {sentiment_label}')\n",
    "\n",
    "# Output (example):\n",
    "# Feature names (vocabulary): ['again' 'amazing' 'buy' 'ever' 'experience' 'good' 'love' 'not' \n",
    "#  'product' 'quality' 'support' 'terrible' 'this' 'will' 'worst']\n",
    "# Document-term matrix shape: (6, 15)\n",
    "# Text Classification Accuracy: 1.0\n",
    "#\n",
    "# Predictions for new texts:\n",
    "# \"This is excellent\" -> Positive\n",
    "# \"I am disappointed\" -> Negative\n",
    "#\n",
    "# Interpretation:\n",
    "# - The CountVectorizer created a vocabulary of 15 unique words from our texts\n",
    "# - Each document is represented as a sparse vector of word counts (6 documents × 15 features)\n",
    "# - Our simple Naive Bayes model correctly classified all training examples (100% accuracy)\n",
    "# - For new texts, the model can predict sentiment even for words it hasn't seen before\n",
    "#   (e.g., \"excellent\" and \"disappointed\") based on the presence of other words\n",
    "# - In real applications, we would use larger training sets and more sophisticated techniques\n",
    "#   like TF-IDF, word embeddings, or transformer models (BERT, GPT, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62de7db",
   "metadata": {},
   "source": [
    "## 5. Model Deployment and Serving\n",
    "\n",
    "Deploying models allows you to serve predictions in real-time or batch mode. Common tools include Flask, FastAPI, and cloud services.\n",
    "\n",
    "**Real-life use case:** Deploying a fraud detection model as a REST API for a banking application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00c35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple Flask API for model prediction (conceptual, not executable in the notebook)\n",
    "import pickle\n",
    "from flask import Flask, request, jsonify\n",
    "app = Flask(__name__)\n",
    "\n",
    "# In a real deployment, you would load your trained model from a file\n",
    "# model_file = 'trained_model.pkl'\n",
    "# with open(model_file, 'rb') as f:\n",
    "#     clf = pickle.load(f)\n",
    "\n",
    "# Define a prediction endpoint\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Get JSON data from the request\n",
    "    data = request.get_json(force=True)\n",
    "    \n",
    "    # Extract features from the request\n",
    "    features = np.array(data['features']).reshape(1, -1)\n",
    "    \n",
    "    # Make prediction using the loaded model\n",
    "    prediction = clf.predict(features)[0]\n",
    "    \n",
    "    # Return prediction as JSON response\n",
    "    return jsonify({'prediction': int(prediction)})\n",
    "\n",
    "# Example JSON request that would be sent to this API:\n",
    "'''\n",
    "{\n",
    "  \"features\": [5.1, 3.5, 1.4, 0.2]\n",
    "}\n",
    "'''\n",
    "\n",
    "# To run the API server (in a real environment):\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(debug=True, host='0.0.0.0', port=5000)\n",
    "\n",
    "print(\"Note: This is a conceptual example of how to deploy a model as a REST API using Flask.\")\n",
    "print(\"In a real deployment scenario, you would:\")\n",
    "print(\"1. Save your trained model to disk using pickle or joblib\")\n",
    "print(\"2. Set up a proper server environment (e.g., Gunicorn, uWSGI)\")\n",
    "print(\"3. Implement error handling and input validation\")\n",
    "print(\"4. Consider containerization with Docker for easier deployment\")\n",
    "print(\"5. Deploy to a cloud platform like AWS, GCP, Azure, or Heroku\")\n",
    "\n",
    "# Output:\n",
    "# Note: This is a conceptual example of how to deploy a model as a REST API using Flask.\n",
    "# In a real deployment scenario, you would:\n",
    "# 1. Save your trained model to disk using pickle or joblib\n",
    "# 2. Set up a proper server environment (e.g., Gunicorn, uWSGI)\n",
    "# 3. Implement error handling and input validation\n",
    "# 4. Consider containerization with Docker for easier deployment\n",
    "# 5. Deploy to a cloud platform like AWS, GCP, Azure, or Heroku\n",
    "#\n",
    "# Key Concepts:\n",
    "# - RESTful API: A standard way to expose your ML model via HTTP endpoints\n",
    "# - Serialization: Converting ML models to a format that can be saved and loaded\n",
    "# - Containerization: Packaging code and dependencies for consistent deployment\n",
    "# - Scaling: Handling multiple prediction requests efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9270e506",
   "metadata": {},
   "source": [
    "## 6. Real-Life Use Cases and Best Practices\n",
    "\n",
    "- **Healthcare:** Deep learning for medical image analysis\n",
    "- **Finance:** NLP for automated document processing\n",
    "- **Retail:** Recommendation systems using collaborative filtering and deep learning\n",
    "- **Manufacturing:** Predictive maintenance with time series and anomaly detection\n",
    "- **Best Practices:**\n",
    "    - Always validate models with cross-validation\n",
    "    - Monitor deployed models for data drift\n",
    "    - Use explainable AI tools (e.g., SHAP, LIME) for model transparency\n",
    "    - Document and version your models for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b8f788",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "1. Try ensemble methods on a different dataset and compare results.\n",
    "2. Build and tune a neural network for a regression problem.\n",
    "3. Perform sentiment analysis on a set of real product reviews.\n",
    "4. Deploy a simple model using FastAPI or Flask.\n",
    "5. Explore explainable AI tools to interpret your model's predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
